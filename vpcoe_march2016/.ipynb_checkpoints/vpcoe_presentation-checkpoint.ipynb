{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Me\n",
    "\n",
    "## Senior Data Scientist @ Sokrati\n",
    "\n",
    "## Advertising Technology, Click Stream and Merchandising Data\n",
    "\n",
    "## Masters Operations Research, IIT Mumbai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image(filename='supervised_learning_basic.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Image from Stack Overflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Learning Using Maths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Hypothesis\n",
    "  $$ \\hat{y} = g (w_{0} + w_{1} * x_{1} + w_{2} * x_{2} + w_{3} * x_{3} + \\dots ) + \\epsilon $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Parameters\n",
    "  $$ w_{0}, w_{1}, \\dots, w_{m} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Error\n",
    "$$ \\frac{1}{n} * \\sum_{i \\in n} (y_{i} - \\hat{y_{i}} ) ^ 2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data\n",
    "  * Is given, constant\n",
    "  \n",
    "## Outputs / Predictions\n",
    "  * Function of parameters\n",
    "  \n",
    "## Parameter Choice\n",
    "  * Results in output (hence errors)\n",
    "  \n",
    "## Make lowest error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parameter Fitting\n",
    "\n",
    "## Choose best w, given the data\n",
    "\n",
    "## How do you choose best?\n",
    "\n",
    "## Intuition : Optimization Routines\n",
    "* Trial and error\n",
    "* Choose w, make prediction\n",
    "* Change w, to improve the accuracy\n",
    "\n",
    "## Decisions to be made\n",
    "* How to calculate accuracy?\n",
    "* How to form initial guess?\n",
    "* How to update w based on accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import random\n",
    "from sklearn.datasets.samples_generator import make_regression \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_theta_0 = []\n",
    "all_theta_1 = []\n",
    "all_cost = []\n",
    "\n",
    "def gradient_descent(alpha, x, y, ep=0.001, max_iter=10000):\n",
    "    converged = False\n",
    "    iter = 0\n",
    "    m = x.shape[0] # number of samples\n",
    "\n",
    "    # initial theta\n",
    "    t0 = np.random.random(x.shape[1])\n",
    "    all_theta_0.append(t0)\n",
    "    t1 = np.random.random(x.shape[1])\n",
    "    all_theta_1.append(t1)\n",
    "\n",
    "    # total error, J(theta)\n",
    "    J = sum([(t0 + t1*x[i] - y[i])**2 for i in range(m)])\n",
    "    all_cost.append(J)\n",
    "\n",
    "    # Iterate Loop\n",
    "    while not converged:\n",
    "        # for each training sample, compute the gradient (d/d_theta j(theta))\n",
    "        grad0 = 1.0/m * sum([(t0 + t1*x[i] - y[i]) for i in range(m)]) \n",
    "        grad1 = 1.0/m * sum([(t0 + t1*x[i] - y[i])*x[i] for i in range(m)])\n",
    "\n",
    "        # update the theta_temp\n",
    "        temp0 = t0 - alpha * grad0\n",
    "        temp1 = t1 - alpha * grad1\n",
    "    \n",
    "        # update theta\n",
    "        t0 = temp0\n",
    "        t1 = temp1\n",
    "        all_theta_0.append(t0)\n",
    "        all_theta_1.append(t1)\n",
    "\n",
    "        # mean squared error\n",
    "        e = sum( [ (t0 + t1*x[i] - y[i])**2 for i in range(m)] ) \n",
    "\n",
    "        if abs(J-e) <= ep:\n",
    "            converged = True\n",
    "    \n",
    "        J = e   # update error \n",
    "        iter += 1  # update iter\n",
    "    \n",
    "        if iter == max_iter:\n",
    "            converged = True\n",
    "\n",
    "    return t0,t1\n",
    "\n",
    "x, y = make_regression(n_samples=100, n_features=1, n_informative=1, \n",
    "                        random_state=0, noise=35) \n",
    " \n",
    "alpha = 0.01 # learning rate\n",
    "ep = 0.01 # convergence criteria\n",
    "\n",
    "    # call gredient decent, and get intercept(=theta0) and slope(=theta1)\n",
    "theta0, theta1 = gradient_descent(alpha, x, y, ep, max_iter=1000)\n",
    "plt.plot(all_theta_0, all_theta_1, marker = \"+\", color = \"red\")\n",
    "plt.figure(1, figsize=(16, 12))\n",
    "plt.ylim(-10, 50)\n",
    "plt.ylabel(\"Parameter $$w1$$\")\n",
    "plt.xlim(-3, 2)\n",
    "plt.xlabel(\"Parameter $$w2$$\")\n",
    "plt.title(\"Gradient descent over parameters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Important Point\n",
    "\n",
    "## X variables need not be numbers\n",
    "### Else, machine learning has limited applications\n",
    "\n",
    "## X variables should be convertible to numbers\n",
    "\n",
    "## Thus, applicable to many fields\n",
    "  * Images\n",
    "  * Text\n",
    "  * Biological Data\n",
    "  * Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text: Focus of Today's talk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples\n",
    "\n",
    "### Sentiment Analysis\n",
    "Same health example as before but with classes as positive negative sentiment.\n",
    "\n",
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### What are the X values?\n",
    "  + What is the part of speech tag of N-1th word?\n",
    "  + What is the part of speech tag of N-2th word?\n",
    "  + What is the location of the phrase in the sentence?\n",
    "  + Is it capitalized?\n",
    "  + What is the part of speech tag of the phrase itself\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image(filename='supervised_learning_basic.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic Idea\n",
    "Words as axis\n",
    "\n",
    "## Improvements\n",
    "  * Frequency Vs. Binary Vs. Modified frequency (TFIDF)\n",
    "  * Cleanups (stop words, numbers, stemming)\n",
    "\n",
    "## Fundamental Issues\n",
    "    * Just BoW may not be enough\n",
    "    * Looses semantic similarity\n",
    "    * Very sensitive to the settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction To Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> -- <cite>All images from Quoc V. Le tutorial on deep learning'</cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGkCAYAAAAv7h+nAAAABHNCSVQICAgIfAhkiAAAFrpJREFU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\nIb4k1LmpM3EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image(filename='nn_intro1.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image(filename='nn_intro2.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image(filename='nn_intro3.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image(filename='nn_intro4.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image(filename='nn_intro5.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image(filename='nn_intro6.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Network Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image(filename='nn_intro7.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Non-Linearity\n",
    "Different activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Training\n",
    "  * Called backpropagation algorithm\n",
    "  * Gradient descent + chain rule of derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Important Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Context Matters\n",
    "  * Time Series\n",
    "  * Images\n",
    "  * Text\n",
    "  * Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Representation Problem\n",
    "Derive a better representation using context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word2Vec Introduction\n",
    "\n",
    "## Pioneers\n",
    "  * Original ideas on distributed layered representations by Hinton et.al. from 1986\n",
    "  * Neural probablistic language model\n",
    "  * Paper on word2vec published by Tomas Mikolov et.al. in 2011-12\n",
    "  * Very actively developed area currently.\n",
    "  \n",
    "## Basic Idea\n",
    "  * Build unsupervised learning\n",
    "  * Exploit contextual information\n",
    "  * Convert words to vectors in continuous space\n",
    "  * Use vectors in classification tasks\n",
    "  \n",
    "## Not a single algorithm\n",
    "2 main models and multiple settings avaialable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Distributional Hypothesis\n",
    "\n",
    "## Residents of city of *Pune* will have to...\n",
    "\n",
    "## Residents of city of *Nagpur* will have to..\n",
    "\n",
    "> Word is known by company it keeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beware.. Maths Ahead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Assume we have vector rep for each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ P(w_t | h) = softmax(score(w_t, h)) \\\\\n",
    "              = \\frac{\\exp\\{ \\text{score}(w_t, h) \\} }\n",
    "             {\\sum_{Word \\, w' \\, \\in \\, Vocab} \\exp\\{ {score}(w', h) \\} } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Score is computed by dot product of vector embeddings\n",
    "* If multiple words in the context, average is taken\n",
    "* Softmax function normalizes the scores to convert to probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural Network Task\n",
    "  * Optimize the vector representation\n",
    "  * To maximize the likelihood for all context, target word combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ J_{ML} = log \\, P(w_t|h) \\\\ \n",
    "          = score(w_t,h)\\, − \\, log(\\sum_{Word \\, w' \\, \\in \\, Vocab} \\exp\\{score(w′,h)\\}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem\n",
    "  + Probability normalization is happening over entire corpus\n",
    "  + Will be very expensive to calculate\n",
    "  \n",
    "## Observation\n",
    "  + Need not maximize the likelihood over all examples\n",
    "  + Probability should be high for real pairs of context, target\n",
    "  + Probability should be low for randomly selected context, target\n",
    "  \n",
    "## Solution\n",
    "  + For every training example seen, use K negative examples\n",
    "  + Negative examples generated through random selection\n",
    "  \n",
    " $$ J_{NEG} = log Q_\\theta(D=1 |w_t, h) +\n",
    "  k \\, \\mathrm{E}_{\\tilde w \\sim P_{noise}}\n",
    "     [ \\log Q_\\theta(D = 0 |\\tilde w, h)] $$\n",
    "     \n",
    "> Q above is the binomial logistic regression probability "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Details Skipped\n",
    "\n",
    "## Two models\n",
    "  + Continuous bag of words and skip gram\n",
    "  + What are the differences?\n",
    "  \n",
    "## Why softmax in output layer\n",
    "\n",
    "## Engineering Implementation\n",
    "  + Optimized data structures for lookups\n",
    "  + Can we speed up using multiple workers/thread?\n",
    "  \n",
    "## How parameters are tuned?\n",
    "\n",
    "## How accuracy is evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Short Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Further Developments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## More Algorithmic Explorations\n",
    "  + Same problem modeled as matrix factorization problem\n",
    "  + Evaluating different optimization criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Handling Polysemy\n",
    "  + If word can have different meanings, why single vector\n",
    "  + I am exploring this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Handling Document Representation\n",
    "  + Tokens build up documents\n",
    "  + Most interesting problems work with documents\n",
    "  + Simple averaging is not enough\n",
    "  + I am exploring this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "> -- <cite>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations\n",
    "in vector space. ICLR Workshop, 2013.</cite>\n",
    "\n",
    "> -- <cite>Tutorial on deep learning by Quoc V. Le, 2015.</cite>\n",
    "\n",
    "> -- <cite>Tutorial for Tensorflow package released open source by Google</cite>\n",
    "\n",
    "> -- <cite>Blog post on word representations by Vered Shwartz  </cite>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
